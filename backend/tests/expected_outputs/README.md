# Expected Outputs Directory

This directory contains the expected outputs and golden files generated by the end-to-end tests.

## File Types

### Filled PA Forms
- `test_case_X_filled.pdf` - Filled prior authorization forms
- `golden_test_case_X_filled.pdf` - Golden reference outputs for regression testing

### Reports
- `test_case_X_report.md` - Missing fields and validation reports
- `golden_test_case_X_report.md` - Golden reference reports

### Validation Results
- `test_case_X_validation.json` - Quality validation results with confidence metrics
- `test_case_X_performance.json` - Performance benchmarks for individual test cases

### Summary Files
- `e2e_test_summary.json` - Comprehensive test results across all test cases
- `golden_outputs_metadata.json` - Metadata about golden reference files

## Generated During Testing

These files are automatically generated when running end-to-end tests:

```bash
# Run comprehensive E2E tests
cd backend
python run_e2e_tests.py --mode comprehensive

# Generate golden outputs for regression testing
python run_e2e_tests.py --mode golden_outputs

# Run performance benchmarks
python run_e2e_tests.py --mode performance
```

## File Structure

```
expected_outputs/
├── README.md                           # This file
├── test_case_1_filled.pdf             # Filled PA form for test case 1
├── test_case_1_report.md               # Missing fields report for test case 1
├── test_case_1_validation.json        # Quality validation results
├── test_case_1_performance.json       # Performance metrics
├── golden_test_case_1_filled.pdf      # Golden reference filled form
├── golden_test_case_1_report.md       # Golden reference report
├── e2e_test_summary.json              # Overall test results
└── golden_outputs_metadata.json       # Golden file metadata
```

## Quality Validation

Each test case generates a validation report that checks:

- **Critical Fields**: Patient name, DOB, member ID, prescriber name
- **Confidence Scores**: Average confidence across all extracted fields
- **Missing Fields Count**: Number of fields that couldn't be mapped
- **Low Confidence Fields**: Fields with confidence < 70%

## Performance Metrics

Performance files track:

- **Processing Time**: Total time for complete pipeline
- **File Sizes**: Input document sizes and output sizes
- **Memory Usage**: Peak memory consumption during processing
- **Throughput**: Pages processed per second

## Regression Testing

Golden outputs serve as reference for regression testing:

1. **Visual Comparison**: Compare filled PDFs visually
2. **Field Accuracy**: Validate field mappings remain consistent
3. **Report Structure**: Ensure reports maintain expected format
4. **Performance Baselines**: Track performance degradation over time

## Cleanup

Test outputs can be cleaned up with:

```bash
# Remove all generated test outputs (keep golden files)
find . -name "test_case_*" -not -name "golden_*" -delete

# Remove all files (including golden references)
rm -rf expected_outputs/*
```